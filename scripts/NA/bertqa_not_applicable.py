# -*- coding: utf-8 -*-
"""
Created on Fri Mar 31 18:58:13 2023

@author: ThinkPad
"""

import textract
from transformers import BertForQuestionAnswering, AutoTokenizer
modelname = 'deepset/bert-base-cased-squad2'

model = BertForQuestionAnswering.from_pretrained(modelname)
tokenizer = AutoTokenizer.from_pretrained(modelname)

from transformers import pipeline
nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)
"""
context = "A quantum computer is a computer that exploits quantum mechanical phenomena. At small scales, physical matter exhibits properties of both particles and waves, and quantum computing leverages this behavior using specialized hardware. Classical physics cannot explain the operation of these quantum devices, and a scalable quantum computer could perform some calculations exponentially faster than any modern 'classical' computer. In particular, a large-scale quantum computer could break widely used encryption schemes and aid physicists in performing physical simulations; however, the current state of the art is still largely experimental and impractical.The basic unit of information in quantum computing is the qubit, similar to the bit in traditional digital electronics. "
context = context + "Unlike a classical bit, a qubit can exist in a superposition of its two 'basis' states, which loosely means that it is in both states simultaneously. When measuring a qubit, the result is a probabilistic output of a classical bit. If a quantum computer manipulates the qubit in a particular way, wave interference effects can amplify the desired measurement results. "
context = context + "The design of quantum algorithms involves creating procedures that allow a quantum computer to perform calculations efficiently. Physically engineering high-quality qubits has proven challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. National governments have invested heavily in experimental research that aims to develop scalable qubits with longer coherence times and lower error rates. Two of the most promising technologies are superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single atomic particle using electromagnetic fields). Any computational problem that can be solved by a classical computer can also be solved by a quantum computer.[2] Conversely, any problem that can be solved by a quantum computer can also be solved by a classical computer, at least in principle given enough time. In other words, quantum computers obey the Church–Turing thesis. This means that while quantum computers provide no additional advantages over classical computers in terms of computability, quantum algorithms for certain problems have significantly lower time complexities than corresponding known classical algorithms. Notably, quantum computers are believed to be able to solve certain problems quickly that no classical computer could solve in any feasible amount of time—a feat known as 'quantum supremacy.' The study of the computational complexity of problems with respect to quantum computers is known as quantum complexity theory."
context = context + " History For a chronological guide, see Timeline of quantum computing and communication. The Mach–Zehnder interferometer shows that photons can exhibit wave-like interference. For many years, the fields of quantum mechanics and computer science formed distinct academic communities.[3] Modern quantum theory developed in the 1920s to explain the wave–particle duality observed at atomic scales,[4] and digital computers emerged in the following decades to replace human computers for tedious calculations.[5] Both disciplines had practical applications during World War II; computers played a major role in wartime cryptography,[6] and quantum physics was essential for the nuclear physics used in the Manhattan Project.[7] As physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, the fields of quantum mechanics and computer science began to converge. In 1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer.[8] When digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics,[9] prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation.[10][11][12] In a 1984 paper, Charles Bennett and Gilles Brassard applied quantum theory to cryptography protocols and demonstrated that quantum key distribution could enhance information security.[13][14] Peter Shor (pictured here in 2017) showed in 1994 that a scalable quantum computer would be able to break RSA encryption. Quantum algorithms then emerged for solving oracle problems, such as Deutsch's algorithm in 1985,[15] the Bernstein–Vazirani algorithm in 1993,[16] and Simon's algorithm in 1994.[17] These algorithms did not solve practical problems, but demonstrated mathematically that one could gain more information by querying a black box in superposition, sometimes referred to as quantum parallelism.[18] Peter Shor built on these results with his 1994 algorithms for breaking the widely used RSA and Diffie–Hellman encryption protocols,[19] which drew significant attention to the field of quantum computing.[20] In 1996, Grover's algorithm established a quantum speedup for the widely applicable unstructured search problem.[21][22] The same year, Seth Lloyd proved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations,[23] validating Feynman's 1982 conjecture.[24] Over the years, experimentalists have constructed small-scale quantum computers using trapped ions and superconductors.[25] In 1998, a two-qubit quantum computer demonstrated the feasibility of the technology,[26][27] " 
context = context + " and subsequent experiments have increased the number of qubits and reduced error rates.[25] In 2019, Google AI and NASA announced that they had achieved quantum supremacy with a 54-qubit machine, performing a computation that is impossible for any classical computer.[28][29][30] However, the validity of this claim is still being actively researched.[31][32] The threshold theorem shows how increasing the number of qubits can mitigate errors,[33] yet fully fault-tolerant quantum computing remains 'a rather distant dream'.[34] According to some researchers, noisy intermediate-scale quantum (NISQ) machines may have specialized uses in the near future, but noise in quantum gates limits their reliability.[34] In recent years, investment in quantum computing research has increased in the public and private sectors.[35][36] As one consulting firm summarized,[37]... investment dollars are pouring in, and quantum-computing start-ups are proliferating. ... While quantum computing promises to help businesses solve problems that are beyond the reach and speed of conventional high-performance computers, use cases are largely experimental and hypothetical at this early stage."
"""
# with open('data1.txt') as f:
#     context = f.read()
#     #print(context)
context = str(textract.process('data1.docx')).replace("\\n", "").replace("\\t","")
print(context)    
for i in range(10):    
    question = input("Ask question: ")    
    result={}
    result = nlp({
         'question': question,
         'context': context
         })
    
    print(result)